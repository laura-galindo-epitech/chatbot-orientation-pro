#!/usr/bin/env python
# coding: utf-8

import os
import json
import getpass

import requests
import chromadb
from chromadb import PersistentClient
from dotenv import load_dotenv

from langchain.vectorstores import Chroma
from langchain_mistralai import ChatMistralAI, MistralAIEmbeddings
from langchain_mistralai.chat_models import ChatMistralAI
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.messages import HumanMessage
from langchain_core.prompts import (
    ChatPromptTemplate, PromptTemplate, PipelinePromptTemplate
)
from langgraph.graph import START, MessagesState, StateGraph
from langgraph.checkpoint.memory import MemorySaver
from langchain import hub


# Load environment variables from .env file

load_dotenv()

LANGSMITH_API_KEY = os.getenv("LANGSMITH_API_KEY")
MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY")
MODEL_NAME = os.getenv("MODEL_NAME")

# Building a Retrieval Augmented Generation (RAG) Chatbot

get_ipython().system('pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph')
get_ipython().system('pip install -qU langchain-mistralai')
get_ipython().system('pip install -qU langchain-chroma')
get_ipython().system('pip install python-dotenv')

# Chat model initialization
llm = ChatMistralAI(
    mistral_api_key=MISTRAL_API_KEY,
    model=MODEL_NAME,
    streaming=True,
    temperature=0.6
)

# Embeddings model
embeddings = MistralAIEmbeddings(model="mistral-embed")

# 1Ô∏è‚É£ Connect to the existing ChromaDB instance
db_path = "./RAG/final_chroma_db"
collection_name = "final_vector_store_collection"

client = PersistentClient(path=db_path)
collection = client.get_or_create_collection(name=collection_name)

# 2Ô∏è‚É£ Load stored embeddings from ChromaDB
stored_data = collection.get()

# 3Ô∏è‚É£ Check how many embeddings are loaded
num_loaded = len(stored_data["ids"])

# üîÑ Recharger le vector store depuis la base de donn√©es ChromaDB
vector_store = Chroma(
    persist_directory=db_path,
    collection_name=collection_name,
    embedding_function=embeddings
)

# LLM Prompt-Engineering

prompt_principal = ChatPromptTemplate.from_template(
    """
    Tu es **Crystal Bot**, un assistant intelligent d'orientation professionnelle.
    Tu dois fournir des r√©ponses bas√©es sur les informations r√©cup√©r√©es.
    
    Ton objectif est d'aider chaque utilisateur √† **clarifier son avenir** en lui proposant **des conseils sur-mesure et motivants**.

    **Contexte pertinent issu de la base de connaissances :** 
    {context}
    
    ‚û§ **Adapte-toi √† l‚Äôutilisateur** : si ind√©cis, propose des pistes d‚Äôexploration.
    ‚û§ **Si comparaison de plusieurs options**, aide-le √† peser le pour et le contre.
    ‚û§ **Propose une r√©ponse fluide, naturelle et pas trop longue**, qui ne se contente pas d‚Äôune liste fig√©e, mais qui √©tablit un dialogue et donne des solutions plus large.
    """
)

topic_prompt = PromptTemplate.from_template(
    """
    Analyse la question suivante et identifie le ou les domaines principaux auxquels elle appartient.

    Question: {input}

    R√©ponds en listant **un ou plusieurs domaines** parmi les suivants (s√©par√©s par une virgule) :
    - "Informatique et Num√©rique"
    - "Sciences et Ing√©nierie"
    - "Sant√© et M√©decine"
    - "Droit et Sciences Politiques"
    - "√âconomie et Gestion"
    - "Sciences Sociales et Psychologie"
    - "Arts, Design et Culture"
    - "Lettres et Langues"
    - "Sciences de l'√âducation"
    - "Communication et Journalisme"
    - "Tourisme, H√¥tellerie et Restauration"
    - "Environnement et D√©veloppement Durable"
    - "Sport et Activit√© Physique"
    - "Architecture et Urbanisme"
    - "Agronomie et Agroalimentaire"

    Si aucun domaine n‚Äôest clair, r√©ponds uniquement par "Autre".

    Domaines identifi√©s :
    """
)

intent_prompt = PromptTemplate.from_template(
    """
    Analyse la question suivante et identifie l‚Äôintention principale de l‚Äôutilisateur :

    Question: {input}

    R√©ponds par l‚Äôune ou plusieurs des options suivantes :
    - "M√©tiers"
    - "Formations"
    - "Salaire"
    - "Offres d'emploi"
    - "Parcoursup"
    - "Autre"

    Intention d√©tect√©e :
    """
)

chatbot_prompt = PromptTemplate.from_template(
    """
    
    Je suis **Crystal Bot**, ton conseiller d'orientation professionnelle. üîÆ‚ú®  
    
    L'utilisateur a demand√© : {input}.
    Domaine d√©tect√© : {topic} (cach√© √† l'utilisateur)
    Intention d√©tect√©e : {intent} (cach√© √† l'utilisateur)
    
  ‚û§ **Si l'intention est "M√©tiers"** :
        - Propose des **m√©tiers adapt√©s** au profil de l‚Äôutilisateur en allant au-del√† des √©vidences.   
        - Explique **les missions, conditions de travail et d√©bouch√©s** en t‚Äôadaptant √† la curiosit√© de l‚Äôutilisateur.  
        - **N‚Äôh√©site pas √† raconter une anecdote ou une tendance actuelle, personnalise la r√©ponse** pour rendre la r√©ponse plus vivante et personnalis√©e.  
        - **Fournis une vision inspirante et dynamique**, et pas seulement une simple liste de m√©tiers.

    ‚û§ **Si l'intention est "Formations"** :
        - Pr√©sente des **formations pertinentes** mais **ajoute des alternatives originales** (double cursus, parcours atypiques).
        - D√©cris les options de mani√®re dynamique : **BTS, Licence, Master, √©coles sp√©cialis√©es, mais aussi MOOC et certifications**.  
        - **Varie les approches selon la question de l‚Äôutilisateur**, sans donner toujours la m√™me liste fig√©e.
        - Rassure l‚Äôutilisateur en expliquant que *sa motivation est un facteur cl√©*. *Donne lui des conseils*
        - **N‚Äôh√©site pas √† raconter une anecdote ou une tendance actuelle, personnalise la r√©ponse** pour rendre la r√©ponse plus vivante et personnalis√©e. 
        - Si pertinent propose √† l'utilisateur de visiter des salons pour en savoir plus sur les formations. 

    ‚û§ **Si l'intention est "Salaire"** :
        - Donne **un aper√ßu d√©taill√© des salaires**, mais va au-del√† des chiffres standards en ajoutant **les tendances du march√©**.  
        - Explique **les diff√©rences selon l‚Äôexp√©rience, le secteur, et la localisation**.  
        - **Si pertinent, parle des avantages indirects** (avantages en nature, perspectives d‚Äô√©volution, travail √† l‚Äôinternational).  
         - **N‚Äôh√©site pas √† raconter une anecdote ou une tendance actuelle, personnalise la r√©ponse** pour rendre la r√©ponse plus vivante et personnalis√©e.  

    ‚û§ **Si l'intention est "Offres d'emploi"** :
        - **D√©cris le march√© actuel** du secteur concern√© : quelles entreprises recrutent ? Quelles comp√©tences sont recherch√©es ?
        - Donne **des conseils personnalis√©s** pour optimiser les candidatures, en fonction du domaine vis√©.
        - **Propose une approche proactive** : comment cr√©er des opportunit√©s, r√©seauter efficacement ou se d√©marquer ?  
         - **N‚Äôh√©site pas √† raconter une anecdote ou une tendance actuelle, personnalise la r√©ponse** pour rendre la r√©ponse plus vivante et personnalis√©e.  

    ‚û§ **Si l'intention est "Parcoursup"** :
        - **Explique Parcoursup en t‚Äôadaptant au profil de l‚Äôutilisateur** (lyc√©en stress√©, √©tudiant en r√©orientation‚Ä¶).
        - Fournis **des conseils concrets pour r√©ussir son dossier** et √©viter les pi√®ges.  
        - Fournis **des conseils concrets pour r√©ussir la lettre de motivation** et se distinguer des autres candidats. 
        - Si pertinent, parle **des alternatives √† Parcoursup** (√©coles priv√©es, √©tudes √† l‚Äô√©tranger, admissions parall√®les).  
    

    ‚û§ **Si l'intention est "Autre"** :
        - **Interpr√®te la demande de l‚Äôutilisateur avec souplesse** et propose une r√©ponse originale.  
        - Si possible, **ouvre la discussion sur des id√©es nouvelles** en fonction de son profil.
        - **Demande lui plus de pr√©cisions sur ce qu'il souhaite apprendre ou approfondir** en terme professionnel ou de formation.
         
    """
)

closing_prompt = PromptTemplate.from_template(
    """
    J‚Äôesp√®re que cette r√©ponse **t‚Äôa aid√© √† y voir plus clair !** üîÆ  
    **As-tu d‚Äôautres questions ?** Je suis l√† pour explorer toutes les possibilit√©s avec toi.  
    """
)

# Setting up conversation memory

memory = MemorySaver()
workflow = StateGraph(state_schema=MessagesState)

def call_model(state: MessagesState):
    """Appelle le mod√®le avec historique de conversation"""
    response = llm.invoke(state["messages"])
    return {"messages": response}

workflow.add_edge(START, "model")
workflow.add_node("model", call_model)
chatbot = workflow.compile(checkpointer=memory)

# Creating the retrieval chain with LangChain

def build_retrieval_chain(vector_store, llm, k=5):
    """
    Build a retrieval chain using a given vector store and LLM
    
    Parameters:
        - vector_store: The vector store containing document embeddings
        - llm: The language model to generate responses
        - k: Number of most relevant document chunks to retrieve (default is 5)
    
    Returns:
        - retrieval_chain: A LangChain retrieval chain object.
    """
    # Create a retriever from the vector store
    retriever = vector_store.as_retriever(search_kwargs={"k": k})
    
    # Load a pre-defined prompt template for retrieval-based Q&A
    retrieval_chat_prompt = hub.pull("langchain-ai/retrieval-qa-chat")
    
    # Create a document combination chain
    combine_docs_chain = create_stuff_documents_chain(llm, retrieval_chat_prompt)
    
    # Create and return the retrieval chain
    return create_retrieval_chain(retriever, combine_docs_chain)

def retrieve_and_format_context(user_query, vector_store, llm):
    """Retrieve documents from vector store and format the context"""
    retrieval_chain = build_retrieval_chain(vector_store, llm)
    retrieved_docs = retrieval_chain.invoke({"input": user_query})

    documents = retrieved_docs["context"]

    formatted_context = "\n".join([doc.page_content for doc in documents])
    sources = [doc.metadata.get("source", "Unknown source") for doc in documents]

    return formatted_context, sources

# Building a full ChatBot response

def generate_response(user_query, vector_store, llm):
    """Generate an optimized response with CrystalBot and keep the history"""

    context, sources = retrieve_and_format_context(user_query, vector_store, llm)
    
    global conversation_history  # Utilisation d'une variable distincte pour stocker les messages

    # Charger l'historique des messages de mani√®re locale
    past_messages = conversation_history if 'conversation_history' in globals() else []  

    # **üîπ √âtape 1 : D√©tection du domaine / topic**
    topic_output = topic_prompt.format(input=user_query)

    # **üîπ √âtape 2 : D√©tection de l‚Äôintention utilisateur**
    intent_output = intent_prompt.format(input=user_query)
    if not intent_output:
        intent_output = "Autre"

    # **üîπ √âtape 3 : G√©n√©ration de la r√©ponse principale**
    chatbot_prompt_hidden = chatbot_prompt.format(
        input=user_query, 
        topic=topic_output, 
        intent=intent_output,
        context=context,
    )

    chatbot_output = chatbot_prompt_hidden.replace(topic_output, "").replace(intent_output, "")

    # **üîπ √âtape 4 : G√©n√©ration des suggestions d‚Äôexploration selon l‚Äôintention d√©tect√©e**
    exploration_suggestions = ""

    if "M√©tiers" in intent_output:
        exploration_suggestions += """
        **üåç Pour aller plus loin :**  
        - üîç **D√©couvre des m√©tiers similaires** via des plateformes comme Onisep ou Studyrama.  
        - üé≠ **Participe √† des salons professionnels** et rencontres avec des experts du domaine.  
        - üë• **√âchange avec des professionnels** sur LinkedIn ou lors d‚Äô√©v√©nements.  
        """

    if "Formations" in intent_output or "Parcoursup" in intent_output:
        exploration_suggestions += """
        **üìö Pour approfondir ton parcours :**  
        - üé§ **D√©couvre les parcours d‚Äôanciens √©tudiants** via des t√©moignages en ligne.  
        - üîé **Explore les formations en alternance** et internationales.  
        - üè´ **Assiste aux journ√©es portes ouvertes** des √©coles et universit√©s.  
        """

    if "Salaire" in intent_output:
        exploration_suggestions += """
        **üí∞ Pour mieux comprendre les salaires et √©volutions de carri√®re :**  
        - üìä **Consulte des √©tudes de r√©mun√©ration** sur Glassdoor et l‚ÄôAPEC.  
        - üìà **Analyse les √©volutions de carri√®re possibles** en fonction de ton secteur.  
        """

    if "Offres d'emploi" in intent_output:
        exploration_suggestions += """
        **üìù Pour trouver des opportunit√©s professionnelles :**  
        - üîç **Consulte des plateformes sp√©cialis√©es** comme Indeed ou P√¥le Emploi.  
        - üì© **Optimise ton CV et ta lettre de motivation** pour te d√©marquer.  
        """

    if "Les deux" in intent_output:
        exploration_suggestions += """
        **üåü Pour lier formations et m√©tiers :**  
        - üìö **D√©couvre les formations qui recrutent le plus**.  
        - üëÄ **Explore les tendances de recrutement dans ton secteur**.  
        """

    if "Autre" in intent_output:
        exploration_suggestions += """
        **üîç Explorons d‚Äôautres pistes !**  
        - ü§î **Pr√©cise un peu plus ta demande**, veux-tu parler de reconversion, d‚Äôentrepreneuriat, d‚Äô√©tudes √† l‚Äô√©tranger ?  
        - üí° **Inspiration** : Parfois, explorer d‚Äôautres secteurs peut ouvrir des portes inattendues.  
        - üîé **D√©couvre des parcours inspirants** : interviews, conf√©rences, podcasts sur des choix de carri√®re atypiques.  
        """

    # **üîπ √âtape 5 : Conclusion dynamique**
    closing_output = closing_prompt.format()

    # ‚úÖ **Ajout d‚Äôune introduction engageante pour la toute premi√®re r√©ponse**
    intro_message = f"""
    Bonjour ! üòä  

    Je suis **Crystal Bot**, ton conseiller d'orientation professionnelle. üîÆ‚ú®  
    Mon objectif est de **t‚Äôaider √† clarifier ton avenir** en te proposant des pistes adapt√©es et personnalis√©es.  
    Voici quelques id√©es et conseils pour t‚Äôaider √† avancer :  
    """

    # ‚úÖ **Cr√©ation du message final √† envoyer au mod√®le**
    final_prompt = f"""
    {intro_message}  

    **üí° Recommandation de Crystal Bot :**  
    {chatbot_output}  

    {exploration_suggestions}  

    {closing_output}  
    """

    # ‚úÖ **Ajout du message dans l'historique**
    messages = past_messages + [HumanMessage(content=final_prompt)]

    # ‚úÖ **Envoi de la requ√™te compl√®te au mod√®le**
    response = llm.invoke(messages)

    # ‚úÖ **Mise √† jour de l'historique de la conversation**
    conversation_history = messages + [response]

    return {
        "response": response.content,
        "sources": sources
    }

def refine_response(chatbot_output, user_query, llm):
    """Improve clarity and precision of response with LLM"""
    refinement_prompt = f"""
    Am√©liore la r√©ponse suivante en la rendant plus pr√©cise et engageante :
    Contexte : {user_query}
    R√©ponse initiale : {chatbot_output}
    R√©ponse am√©lior√©e :
    """

    refined_response = llm.invoke([HumanMessage(content=refinement_prompt)]).content

    return refined_response