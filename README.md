import requests
import csv
import time
from bs4 import BeautifulSoup

# URL de la page principale des mÃ©tiers
BASE_URL = "https://www.cidj.com/metiers"
HEADERS = {"User-Agent": "Mozilla/5.0"}

def get_metiers_urls():
    """RÃ©cupÃ¨re les liens des fiches mÃ©tiers sur plusieurs pages"""
    metiers_data = []

    for page in range(0, 5):  # Adapter le nombre de pages si besoin
        url = f"{BASE_URL}?page={page}"
        response = requests.get(url, headers=HEADERS)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            # SÃ©lecteur mis Ã  jour pour les titres des mÃ©tiers
            metiers = soup.find_all("h3", class_="node-title")  # VÃ©rifie la classe des titres

            for metier in metiers:
                titre = metier.text.strip()
                lien = "https://www.cidj.com" + metier.a["href"]
                print(titre, lien)  # Affiche les rÃ©sultats pour vÃ©rifier
                metiers_data.append((titre, lien))

            print(f"âœ… Page {page} scrappÃ©e avec succÃ¨s !")
        else:
            print(f"âŒ Erreur {response.status_code} sur la page {page}")

        time.sleep(2)
    
    return metiers_data

def scrape_fiche_metier(url):
    """Scrape la description d'un mÃ©tier Ã  partir de son URL"""
    response = requests.get(url, headers=HEADERS)
    
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, "html.parser")
        # SÃ©lecteur mis Ã  jour pour la description
        description_tag = soup.find("div", class_="field-body")  # VÃ©rifie la classe de la description

        if description_tag:
            return description_tag.get_text(strip=True)
    
    return "Description non disponible"

def save_to_csv(metiers_data, filename="metiers_cidj.csv"):
    """Enregistre les donnÃ©es des mÃ©tiers dans un fichier CSV"""
    with open(filename, "w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["MÃ©tier", "Lien", "Description"])  # En-tÃªte
        
        for titre, lien in metiers_data:
            description = scrape_fiche_metier(lien)
            writer.writerow([titre, lien, description])
            print(f"ğŸ“„ MÃ©tier sauvegardÃ© : {titre}")
            time.sleep(2)  # Pause entre chaque fiche mÃ©tier
    
    print(f"âœ… DonnÃ©es enregistrÃ©es dans {filename}")

if __name__ == "__main__":
    print("ğŸ” Scraping des mÃ©tiers en cours...")
    metiers = get_metiers_urls()
    
    print(f"ğŸ“Œ {len(metiers)} mÃ©tiers trouvÃ©s !")
    save_to_csv(metiers)
    print("ğŸ‰ Scraping terminÃ© avec succÃ¨s !")
